There are some "vanity metrics" that don't make sense or make less sense for e.g. a marketing site or a web app (pages / visit, total visits, visits from social media) because they give you a general idea of your brand equity and also because, simply put, every pageview is a source of revenue so there's no "funnel" to go through at all, no ultimate goal beyond having people read stuff and share stuff.

But still, we need to think way more about what we measure and how.

---

The big thing to realize is that actionable metrics are different for media than they are for startups, and Google Analytics stats make more sense for us than they do for a Silicon Valley start-up. But that doesn't mean we should just accept the stats we get. I've been breaking my head about what it means to apply Eric Ries's lessons about actionable vs. vanity metrics to the media industry. It's not always obvious how they do (even though I believe that they do!)

* For organizations selling e-books, subscriptions etc., the same advice about conversation rates and funnels does apply. Nothing specific to the media industry there. I'm betting most news organizations don't do funnel analysis of their subscription process. Do people find their way to the subscription page, when they do, do they proceed to the payment stage, and once there, do they actually subscribe or decide against it? Lots of newspapers have really awkward online subscription forms, the kind that embarrass marketers in any other industry.
* per month smooths out differences that are outside of our control and hence unmerited (you'll have stories that really resonate every once in a while, maybe even go viral; if you publish enough, that sort of thing will happen roughly every month so it should stop producing big spikes in your stats -- meaningless spikes that you don't benefit from seeing anyway) Avoid per week, per day and live analytics like the plague (*unless* you will actually use it to act on right now: e.g. see what people are looking at and searching for, and expand on that topic; having live metrics for eye candy is preposterous)
* in general, remove noise. And "noise" actually consists of things you wouldn't expect to be noise even though they are. Traffic spikes because of an unexpected viral story: noise. Traffic spike because it just keeps on snowing and people want to know whether they'll be able to get to work: noise. Fewer visitors in the summer because people are on vacation: noise. Any swing in a metric that is not a reflection of *your* performance is noise.
* more fine-grained means it's easier to see cause and effect (e.g. of a new social media strategy). So per-section and per-author. There's a saying that you shouldn't translate your organizational structure to your website ("show your drawers") but for analytics, you explicitly should. If health and science is a single desk, then that's a group you want to track. Pieces written by freelancers should get a group too, to see how they compare.
* I wonder if you can get proxy measures for story effectiveness. Pageviews and tweets, obviously, but maybe also completion rate (how many people read to the end)? Something else?
* While I'm really enamored with the idea of measuring impact, I don't think it's a good idea to try and quantify that. But maybe we can combine quantitative metrics that measure mindshare/measurability with heuristics to determine qualitatively how well we think a story did, e.g. with a standardized five-question checklist or questionnaire (Are you happy with the attention this story received? Do you think most people understood what it meant and why this is important? Should we follow up?) Doesn't matter that those things are a bit vague sometimes. "Measure things to improve them" still applies, but there is absolutely no reason to try and reduce those measurement to hard numbers that only tell half the story anyway. So maybe we should mix those together?
* As Daniel Bachhuber suggested, maybe you can poll your readers and ask them simple questions sometimes, do determine how many people in your audience know the basics of a certain issue. (Or, as Daniel suggested, use it to determine whether people see or don't see intro grafs to a story.)
* create custom dashboards for specific goals: it makes sense to measure pages per visit when you're testing out better ways of showing related content. It does not make sense to look at pages per visit on a regular basis outside of that context. It makes sense to track your bounce rate from social media visits if you're reorganizing your story pages to tease people to stick around after they've read that one specific story someone recommended on Facebook. It does not make sense to track bounce rate without any goal in mind.
* Specific metrics: bounce rate on stories from social media visits originating from within Iowa, *not* bounce rate on every page for every visitor.
* Improving the story mix can be really important too: what stories really resonated with people, which ones don't, and are there commonalities? But again, this is not something you should track on an ongoing basis per sÃ©. It makes sense to know what resonates if you do it live (and thus can respond, or decide what goes in the printed paper) and it makes sense to do an analysis once or twice a year or whenever there's a discussion about editorial priorities, but there's no point in really "tracking" it.
* Writers should be able to compare their performance to their peers. I don't like the idea of editors harassing reporters when their stats aren't up to snuff, but maybe if a reporter sees that he's doing very poorly on social media compared to his colleagues, that might be something that reporter will think about and do something about.
* It might be interesting to track professional metrics (Did I feel I had enough time to write this? How much time was that? How good do I think this story is?) and then compare that with business metrics (pageviews, shareability) and look at the overlap (how well does an author's confidence in their piece translate into success, and if not, can we take a guess at what's holding them back?)
* Pageviews actually are important because that's where your money comes from. Works even better though when you put it alongside the number of pageviews you've been able to sell plus the ones where you had backfill, and then calculate the profits for each. You might end up deciding that there's a certain amount of pageviews you can reliably sell, and so anything below that is dangerous (loses you money), but anything above that isn't really important, so beyond that you might as well optimize for long-term health or diversify your offering and introduce other moneymakers (e-books, events, whatever.) And then couple that with sales stats, so you know if sales are trending upward, which will eventually lead your "minimum acceptable amount of pageviews" to rise.
--> The history of online video journalism provides us with a good example. In the early days of the web you couldn't convince enough advertisers to run their ads on web video, so you could produce as much video as you wanted, it would never pay for itself. Then traditional advertisers started getting on board with the idea of running broadcast ads as pre-rolls, but most news consumption was happening at work, and so it was hard to generate enough pageviews to run ads against. Only now does that audience seem to have grown enough for audience demand and advertiser demand to be in sync.
* Avoid proxy metrics unless you can't avoid them, or unless direct tracking is an inordinate amount of effort. Amount of followers on twitter is a proxy measure for how often your content will get shared, which in itself is a proxy measure for how many pageviews and potential new visitors you can get out of social media. So the important thing to track is (1) the amount of visitors you get from social media and (2) the funnel from followers on Twitter and fans on Facebook to visitors. You'll want to know what your tweets/retweets per follower per month rate is, and the amount of visits  per tweet, so you can see if there's room for improvement there (e.g. lots of people follow you but they hardly ever retweet -- maybe you should tweet more/less/at a different time?)
* You can actually do cohort analysis on anonymous users, as long as they don't wipe their cookies. Meaning: you can check to see how many first-time visitors turn into loyal visitors (and whether that's improving or going down over time), how many never come back, and how many come back but only occassionally. And then look at how they use the site, and see if there's any patterns. E.g. social media visits make people more loyal or less loyal visitors (and thus social media is more or less valuable as a marketing mechanism)
* Avoid doing experiments that will influence each other at the same time. You can improve time on site by making your site a maze to navigate, and you can improve time on site by creating better content that's really engaging. Doing a full site redesign won't allow you to figure out what happens. Changing the navigation first, and the story page some other time *will* tell you what's having an impact.
* Another way to get more clarity about cause and effect is through AB-tests: does time on page improve when you add widget X to the story page, or when it's absent. What's the effect of having many links in a story or no links at all. Google Analytics doesn't give you this, but software like KISSMetrics can.
* Another aspect is effectiveness. You could track pageviews per word (for a story), or, less mercenary, measure whether long-form content attracts and retains new visitors more than linkbait does, and if so to what extent. Tools like "Tweriod":http://www.tweriod.com tell you how to time your tweets if you want the biggest amount of visibility.
* Getting users to log in is nice not necessarily because it gives you better analytics (almost nobody wipes their cookies so you can track anonymous users pretty reliably, albeit not across different devices) but chiefly because it gives you more opportunities to *act* on that information, e.g. by sending lifecycle emails or by following up with people who have stopped coming back
* Google Analytics *is* useful for learning more about your audience: what browser and what screen sizes they use (and thus which ones you need to test), how long it takes for pages to load (even better: correlate that with pages/visit.) Essentially, audience capabilities rather than audience behavior.
* Metrics can sometimes guide editorial decisions too. What if, for every story you've ever done, you can instantly pull up an estimate of how many people in your audience have seen it? You can use that to determine what level of pre-existing knowledge to assume while you're writing. If you have per-visitor stats (and again, you can uniquely identify anonymous readers too) you can even show/hide an intro paragraph based on that user's history. Or you can set goals for how many people should know about an important story, and just keep writing about it until you reach that quota.
* Web app metrics often contain something like score cards for users, based on how a user interacts with the website. Do they use feature X, do they use the app multiple times a week, have they invited other users to use the application, how long did it take them to convert from a trial into a paid plan and so on.

(Philly.com tries to do something like that, -- http://www.niemanlab.org/2010/10/getting-beyond-just-pageviews-philly-coms-seven-part-equation-for-measuring-online-engagement/ -- but unfortunately, making a sum of aggregated engagement metrics is the wrong way to do this: if 50% of your users post comments and 30% share your content on Twitter, you don't have 40% engagement but anywhere between 30% and 80% depending on the overlap between those two groups and what your treshold for engagement is (satisfying just one criterion is too weak, but satisfying all of them an unreasonable expectation). So that's not how the math works. You need to make the sum of each engagement metric for each individual user and then graph those scores as a cumulative distribution function. That's the only thing that will give you an accurate view of how many truly engaged users you have as well as what the distribution is from a tiny bit engaged to a lot.)

* Don't just average everything out. Distribution is really important. Google Analytics gives you some distributions (it does so for page load times and 
* It's all about statistics. If you have a hunch that a certain metric might have an impact on revenue, prove it, don't just assume it. For example, find out if there's a correlation between page load time and pages/visit, and based on how strong that correlation is, prioritize (or deprioritize) site optimization.
* Change things up to attract new users, but be careful not to throw away the things that attract your core audience in the first place. Metrics can tell you what to fix but they can also tell you what you're dealing with. An audience that predominantly only visits 2 or 3 pages a visit, that may be something you desperately need to change but it might also be just the kind of user you're dealing with (and have to deal with!) and instead you should worry about improving other metrics by creating a better experience for the users you have. A/B-tests can sometimes show you whether a different experience attracts a different sort of user (and chases away another kind of user) and then you can analyze the attractiveness of each audience, and decide from there what audience to prioritize. For example, you may be obsessing about getting more pages per visit, but maybe that's not how it works, maybe lots of short visits from a loyal audience is much more valuable than hour-long visits from people who check back irregularly.
---

