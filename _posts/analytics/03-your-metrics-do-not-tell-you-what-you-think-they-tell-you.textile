---
title: Your metrics don't tell you what you think they tell you
language: en
layout: post
categories:
    - journalism
    - business & management
summary: We've gotten used to winging it when it comes to metrics. Some tips for how to do better.
---

h3. 4. Can you separate signal from noise?

The best metrics go up and down together with your performance. Unfortunately, most metrics aren't like that: they're full of noise and unexpected spikes that tell you nothing about whether you're doing a good job.

  - make sure metrics reflect your performance, not luck or unexpected spikes
  (those aren't actionable and they don't reflect how well you're doing, so you
  want to avoid them influencing the stats as much as possible)
  - per-customer data (time on site, time on page, visits/month, churn rate) are better than aggregate data because they tell you how well your product is working in a way that ignores growth -- which is a good thing. (Marketing guys and growth hackers should care about growth, product dev guys should care about engagement and churn.) For example, if you make changes to your product and couple that with a big marketing push, you might double your amount of visitors, but maybe they just visit once or twice and then leave, and meanwhile the changes to your product are alienating your core audience. If you look at your total amount of visitors for that month, it'll be a huge success. In reality, it's an utter failure. And when you track the success of that marketing campaign, you should look at the amount of new but returning visitors it got you (and paying visitors if you have a paywall) and whether your customer acquisition cost was acceptable. How many hits it got you is worse than meaningless, it's misleading. --- Eric Ries: "even a big rush of new customers shouldn’t change how many pages they each view on average, unless you’re getting a new kind of customer." http://www.fourhourworkweek.com/blog/2009/05/19/vanity-metrics-vs-actionable-metrics/
  - slower metrics (monthly)

* per month smooths out differences that are outside of our control and hence unmerited (you'll have stories that really resonate every once in a while, maybe even go viral; if you publish enough, that sort of thing will happen roughly every month so it should stop producing big spikes in your stats -- meaningless spikes that you don't benefit from seeing anyway) Avoid per week, per day and live analytics like the plague (*unless* you will actually use it to act on right now: e.g. see what people are looking at and searching for, and expand on that topic; having live metrics for eye candy is preposterous)

* Not everything needs to be tracked. ---- e.g.  Improving the story mix can be really important too: what stories really resonated with people, which ones don't, and are there commonalities? But again, this is not something you should track on an ongoing basis per sé. It makes sense to know what resonates if you do it live (and thus can respond, or decide what goes in the printed paper) and it makes sense to do an analysis once or twice a year or whenever there's a discussion about editorial priorities, but there's no point in really "tracking" it.

  - filtered or very specific metrics (e.g. only in-state traffic; not bounce rate but bounce rate from social media visits... and only if you're reorganizing your story pages)
* Specific metrics: bounce rate on stories from social media visits originating from within Iowa, *not* bounce rate on every page for every visitor.
  - potential garbage visits or pageviews to get rid of: those from out of state or out of country, those that last less than 15 seconds, ...

* in general, remove noise. And "noise" actually consists of things you wouldn't expect to be noise even though they are. Traffic spikes because of an unexpected viral story: noise. Traffic spike because it just keeps on snowing and people want to know whether they'll be able to get to work: noise. Fewer visitors in the summer because people are on vacation: noise. Any swing in a metric that is not a reflection of *your* performance is noise.

h3. 5. Did you really find cause and effect?

Dirty, noise metrics make it hard to figure out cause and effect. But even clean metrics, like time on page, can be excruciatingly hard to interpret. Google has a rather low time on site. After all, people keep clicking away as soon as they've found what they want. Is that a problem? Of course not! They keep coming back, and that's all that matters. Daily actives, searches per user per day, those are unequivocally good things, not open to ten different interpretations.

  - A key rule of good web design is to not model your navigation on users' needs, not your company's departments. In analytics reports, though, you should follow the grain of your organization (how is our sports team doing? how is this individual author doing? What happens when she changes her twitter strategy?)
  - be fine-grained
  - do A/B experiments
  - avoid doing multiple experiments / projects at the same time if they'll influence each other


h3. 8. Are you tracking what you think you're tracking?

Let's do some exercises.

* Time on site, pages per visit are really ambiguous. For example, Google has a very low time on site and pages per visit, because people usually find what they need to fairly quickly. Analogously, maybe people's visits to thegazette.com are fairly short because that's simply how people like to consume the news: two, three times a day, in short bursts. Time on page is similarly ambiguous: are people reading deeper into stories or did a redesign increase time on page because we've made it clunkier, or are we simply writing longer stories?

For everything we track, we want to look at how it relates to revenue, but we do want to track things beyond _revenue itself_, namely whatever is predictive of future revenue, directly or indirectly.

--- note: I'm already using some of this content in post 02. It's okay to reuse this but I should refer back to that post and say that I'm expanding on an example I used previously. ---

When you're tracking return visitors, what are you getting at? How many loyal users you have? If so, what we really want to track is DAU (daily active users). And _how_ loyal they are? Calculate churn as the percentage of users that haven't returned to your site in the last month. How addictive your product is? The DAU/MAU ratio (daily active users / monthly active users). How alluring to new users? Whether you're able to lure in new users? Bounce rate for first-time visitors from social media or do a cohort analysis.

Why those four metrics instead? Because looking at return visitors tells you nothing in itself. But the amount of loyal users you have can help you predict your customer base for premium offerings or events. How loyal they are can help you decide what your marketing and social media budget should be. And if your website doesn't manage to keep new users coming back, then you're throwing marketing dollars away. The stickiness of your website tells you how good your news site and its writing is at keeping people around, which is essential to customer lifetime value, and it's usually easier and cheaper to keep your existing users happy than it is to find new ones.

The vast majority of people that go to news sites go to them daily, so if they don't visit your site daily (or, if you're a niche site, maybe weekly) then that may be untapped potential.

Let's do another exercise. Followers vs. tweets vs. clicks on tweets vs. stickiness after clicks on tweets + funnel analysis


Another exercise. Time on site is a proxy for pageviews/visit which in turn is a proxy for pageviews/user/month. (If you want to make money, who cares about pages/visit? It's about totals, not the average per visit. The exception would be if,  after thorough testing, you've found focusing on pages/visit to be the only reliable way to boost pageviews/user/month.) Only in limited cases does it make sense to look at pageviews/visit, e.g. together with bounce rate they can be good measures when you're implementing changes to your related content widgets and need to know if they improve stickiness or not.


When using proxies, 
  - see the difference between proxy measures that are an acceptable substitute and those that measure something that's so indirectly related to what we really care about that we might as well not measure it at all. Sometimes nothing is better than something. Time on page might be a good example, unless within the context of a specific experiment (ideal story length for a run-of-the-mill news update.)
  - Try to connect everything to revenue and things close to revenue: (lifetime value, loyalty, conversions), performance (pageviews/reporters) or expenses (how many loyal customers did your latest marketing push deliver.)
  - metrics make sense when they're hard (very close to revenue or indeed revenue indicators themselves) or predictive (not so close to revenue, but they tell you something new), e.g. sentiment about your brand on social media is a really poor proxy for business performance (e.g. #nbcfail), but it might affect the long-term health of your business in a way that pageviews or revenue trends can't

* Don't pick a random metric and convince yourself you're tracking engagement (or whatever.) Be specific and make sure you're really measuring what you think you're measuring.



* Avoid proxy metrics unless you can't avoid them, or unless direct tracking is an inordinate amount of effort. Amount of followers on twitter is a proxy measure for how often your content will get shared, which in itself is a proxy measure for how many pageviews and potential new visitors you can get out of social media. So the important thing to track is (1) the amount of visitors you get from social media and (2) the funnel from followers on Twitter and fans on Facebook to visitors. You'll want to know what your tweets/retweets per follower per month rate is, and the amount of visits  per tweet, so you can see if there's room for improvement there (e.g. lots of people follow you but they hardly ever retweet -- maybe you should tweet more/less/at a different time?)
* You can actually do cohort analysis on anonymous users, as long as they don't wipe their cookies. Meaning: you can check to see how many first-time visitors turn into loyal visitors (and whether that's improving or going down over time), how many never come back, and how many come back but only occassionally. And then look at how they use the site, and see if there's any patterns. E.g. social media visits make people more loyal or less loyal visitors (and thus social media is more or less valuable as a marketing mechanism)
* Avoid doing experiments that will influence each other at the same time. You can improve time on site by making your site a maze to navigate, and you can improve time on site by creating better content that's really engaging. Doing a full site redesign won't allow you to figure out what happens. Changing the navigation first, and the story page some other time *will* tell you what's having an impact.
* Another way to get more clarity about cause and effect is through AB-tests: does time on page improve when you add widget X to the story page, or when it's absent. What's the effect of having many links in a story or no links at all. Google Analytics doesn't give you this, but software like KISSMetrics can.