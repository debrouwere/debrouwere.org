---
title: Think like a statistician
language: en
layout: post
categories:
    - journalism
    - business & management
summary: Think like a statistician, or, your metrics don't tell you what you think they tell you.
---

This poll is bullshit. That longitudinal study doesn't make any sense. I bet you've never thought about these confounding variables, but they make your precious theory void. When we hear about a statistician, it's usually because they've just told someone that _they're wrong_. And I kind of like that. In fact, I think we could all use a little bit more of that skepsis.

Here's three questions I figure a statistician would ask of his data, and some ways to do better.

h3. Can you separate signal from noise?

The best metrics go up and down together with your performance. Unfortunately, most metrics aren't like that: they're full of noise and unexpected spikes that tell you nothing about whether you're doing a good job.

  - make sure metrics reflect your performance, not luck or unexpected spikes
  (those aren't actionable and they don't reflect how well you're doing, so you
  want to avoid them influencing the stats as much as possible)
  - per-customer data (time on site, time on page, visits/month, churn rate) are better than aggregate data because they tell you how well your product is working in a way that ignores growth -- which is a good thing. (Marketing guys and growth hackers should care about growth, product dev guys should care about engagement and churn.) For example, if you make changes to your product and couple that with a big marketing push, you might double your amount of visitors, but maybe they just visit once or twice and then leave, and meanwhile the changes to your product are alienating your core audience. If you look at your total amount of visitors for that month, it'll be a huge success. In reality, it's an utter failure. And when you track the success of that marketing campaign, you should look at the amount of new but returning visitors it got you (and paying visitors if you have a paywall) and whether your customer acquisition cost was acceptable. How many hits it got you is worse than meaningless, it's misleading. --- Eric Ries: "even a big rush of new customers shouldn’t change how many pages they each view on average, unless you’re getting a new kind of customer." http://www.fourhourworkweek.com/blog/2009/05/19/vanity-metrics-vs-actionable-metrics/
  - slower metrics (monthly)
  - embrace the fact that even with monthly metrics, changes that are less than 5-10% and are not sustained over a period of many months, you might well be looking at statistical noise. A metric like unique visitors can drop or increase ten or even twenty percent through a confluence of different factors rather than whatever changes you made to the site. This is why AB-tests are so valuable: they cut out a lot of noise.

* per month smooths out differences that are outside of our control and hence unmerited (you'll have stories that really resonate every once in a while, maybe even go viral; if you publish enough, that sort of thing will happen roughly every month so it should stop producing big spikes in your stats -- meaningless spikes that you don't benefit from seeing anyway) Avoid per week, per day and live analytics like the plague (*unless* you will actually use it to act on right now: e.g. see what people are looking at and searching for, and expand on that topic; having live metrics for eye candy is preposterous)

* Not everything needs to be tracked. ---- e.g.  Improving the story mix can be really important too: what stories really resonated with people, which ones don't, and are there commonalities? But again, this is not something you should track on an ongoing basis per sé. It makes sense to know what resonates if you do it live (and thus can respond, or decide what goes in the printed paper) and it makes sense to do an analysis once or twice a year or whenever there's a discussion about editorial priorities, but there's no point in really "tracking" it.

  - filtered or very specific metrics (e.g. only in-state traffic; not bounce rate but bounce rate from social media visits... and only if you're reorganizing your story pages)
* Specific metrics: bounce rate on stories from social media visits originating from within Iowa, *not* bounce rate on every page for every visitor.
  - potential garbage visits or pageviews to get rid of: those from out of state or out of country, those that last less than 15 seconds, ...

* in general, remove noise. And "noise" actually consists of things you wouldn't expect to be noise even though they are. Traffic spikes because of an unexpected viral story: noise. Traffic spike because it just keeps on snowing and people want to know whether they'll be able to get to work: noise. Fewer visitors in the summer because people are on vacation: noise. Any swing in a metric that is not a reflection of *your* performance is noise.

h3. How do you know you have cause and effect figured out?

Dirty, noise metrics make it hard to figure out cause and effect. But even clean metrics, like time on page, can be excruciatingly hard to interpret. Google has a rather low time on site. After all, people keep clicking away as soon as they've found what they want. Is that a problem? Of course not! They keep coming back, and that's all that matters. Daily actives, searches per user per day, those are unequivocally good things, not open to ten different interpretations.

  - A key rule of good web design is to not model your navigation on users' needs, not your company's departments. In analytics reports, though, you should follow the grain of your organization (how is our sports team doing? how is this individual author doing? What happens when she changes her twitter strategy?)
  - be fine-grained
  - do A/B experiments
  - avoid doing multiple experiments / projects at the same time if they'll influence each other


h3. Are you tracking what you think you're tracking?

Any piece of analytics software does some things well, some things badly and some things not at all. It's deceptively easy to get into the habit of picking metrics because they're available, and then pretend they're really the kind of numbers we need after all.

Say you've decided that it's really important to have a loyal audience, to have true fans. Okay, so how do you identify those?

If you're like most people, you're probably thinking time on site and pages per visit are pretty darn good indicators of a true fan: people who just can't stop browsing. Maybe. Maybe not. Google's happiest users are those who stick around for the least amount of time: they always find what they need in a heartbeat. Similarly, maybe a true fan doesn't spend any more time on our site than anyone else, they just visit more often, and what you were really measuring were people who are just really, really bored at work.

Lots of metrics are ambiguous like that. Notoriously, slideshows are a great way to increase pageviews, but if you think that's a _good_ thing, you might want to guess again.

Let's expand on an example from a previous post. You want to know how addictive your website is, so you track return visitors. Well, think again. Return visitors can mean anything. The DAU/MAU ratio, that is, the ratio of daily active users and monthly active users, will tell you much more. But that might still tell you nothing about how alluring your site is to new users. Maybe DAU/MAU just shows you that you're stuck with a bunch of regulars glued to their bar stools. So adding bounce rate for first-time visitors to the metric mix might give you a clearer picture.

When you go ahead make changes that might upset the regulars, you're going to want to do a cohort analysis so you can see the effect of your actions on newbies and ancients separately.

If you don't want to read things into your metrics that aren't there, it pays to be very precise.

If you want to get a vague estimate of how many of your visitors would maybe buy a ticket to an event, pageviews are not what you want. The amount of in-state visitors that visit every day is still rough, but maybe closer to your real addressable audience.

Another exercise. You care about your presence on social media. First question: why? Is it making you any money? Well, it can if your followers on Twitter return to your site more often. (But think of cause and effect, though! Do your most loyal visitors follow you on Twitter because they love you so much, or have they become your most loyal visitors through your exceptional tweets?)

Still, let's dig a little bit deeper. Ultimately, we want more visitors and visitors that stick around for longer. Does that equate to followers? Nope. Maybe retweets? Nope. Maybe clicks on tweets and retweets? Closer. How long it takes the average first-time visitor from social media to become the kind of guy who actually bookmarks our site or types in the URL in his browser? Bingo, now that's something we can work with.

One last exercise.

Another exercise. Time on site is a proxy for pageviews/visit which in turn is a proxy for pageviews/user/month. (If you want to make money, who cares about pages/visit? It's about totals, not the average per visit. The exception would be if,  after thorough testing, you've found focusing on pages/visit to be the only reliable way to boost pageviews/user/month.) Only in limited cases does it make sense to look at pageviews/visit, e.g. together with bounce rate they can be good measures when you're implementing changes to your related content widgets and need to know if they improve stickiness or not.

Ultimately, all metrics are proxies for revenue. More _x_ helps your business or it doesn't. But revenue itself is a black box, it can swing for reasons that have nothing to do with how much ass you're kicking and it's hard to figure out cause and effect. That's why we have to take apart the strands, think about everything that could cause us to do better or worse. Things that are early predictors and warning signs. That's what makes a proxy worth tracking. But that means your proxies can't be black boxes themselves. They're attractive only insofar as they are stable (not noisy), predictive and insofar as they make it easy for you to tell _what_ is working and what isn't. (Ultimately, that's why AB-testing is so important: it's like magic in how it can turn noisy metrics into clean ones and show you clearly how _one_ factor and one factor only is affecting people's behavior.)


When using proxies, 
  - see the difference between proxy measures that are an acceptable substitute and those that measure something that's so indirectly related to what we really care about that we might as well not measure it at all. Sometimes nothing is better than something. Time on page might be a good example, unless within the context of a specific experiment (ideal story length for a run-of-the-mill news update.)
  - Try to connect everything to revenue and things close to revenue: (lifetime value, loyalty, conversions), performance (pageviews/reporters) or expenses (how many loyal customers did your latest marketing push deliver.)
  - metrics make sense when they're hard (very close to revenue or indeed revenue indicators themselves) or predictive (not so close to revenue, but they tell you something new), e.g. sentiment about your brand on social media is a really poor proxy for business performance (e.g. #nbcfail), but it might affect the long-term health of your business in a way that pageviews or revenue trends can't

* Don't pick a random metric and convince yourself you're tracking engagement (or whatever.) Be specific and make sure you're really measuring what you think you're measuring.



* Avoid proxy metrics unless you can't avoid them, or unless direct tracking is an inordinate amount of effort. Amount of followers on twitter is a proxy measure for how often your content will get shared, which in itself is a proxy measure for how many pageviews and potential new visitors you can get out of social media. So the important thing to track is (1) the amount of visitors you get from social media and (2) the funnel from followers on Twitter and fans on Facebook to visitors. You'll want to know what your tweets/retweets per follower per month rate is, and the amount of visits  per tweet, so you can see if there's room for improvement there (e.g. lots of people follow you but they hardly ever retweet -- maybe you should tweet more/less/at a different time?)
* You can actually do cohort analysis on anonymous users, as long as they don't wipe their cookies. Meaning: you can check to see how many first-time visitors turn into loyal visitors (and whether that's improving or going down over time), how many never come back, and how many come back but only occassionally. And then look at how they use the site, and see if there's any patterns. E.g. social media visits make people more loyal or less loyal visitors (and thus social media is more or less valuable as a marketing mechanism)
* Avoid doing experiments that will influence each other at the same time. You can improve time on site by making your site a maze to navigate, and you can improve time on site by creating better content that's really engaging. Doing a full site redesign won't allow you to figure out what happens. Changing the navigation first, and the story page some other time *will* tell you what's having an impact.
* Another way to get more clarity about cause and effect is through AB-tests: does time on page improve when you add widget X to the story page, or when it's absent. What's the effect of having many links in a story or no links at all. Google Analytics doesn't give you this, but software like KISSMetrics can.


---

* playing devil's advocate can be very powerful when you want to draw conclusions from data. Try to explain it any other way.


---

h3. 6. Have you thought about the right level of detail?

Not cutting up data in the right way can hide important information or even give false impressions

You can vastly increase how actionable a certain metric is by aggregating or splitting up data in the right way.

  - averages can be misleading; don't forget about distributions (Do you have really loyal people among your visitors, or is everyone sort of half-interested? An average won't tell you that, but the revenue potential for those two groups and the marketing techniques you'd use to reach them and product priorities are very different depending on the makeup of your audience. Also: if a lot of your readers obsess about sports and another half obsesses about politics, if you'd use just averages your reader persona would be someone who reads a 50/50 mix between sports and politics. That's worse than misleading.)
  - sometimes you need cohorts to see how changes impact *new* users rather than your total user base
  - knowing your churn rate is interesting, but it's much less interesting than being able to find common patterns in users that will likely not become return visitors, or subscribers that are likely to not renew their subscription, and then to act on that information

Some analytics software allows you to download raw metrics data. Sometimes running a statistical analysis on a big dataset is the only way to find the answers you're looking for.