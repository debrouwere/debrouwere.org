h3. 7. Each story is an individual product, each reporter her own team

You need the right level of detail, and one part of that is the right level of granularity.

One thing to remember is that, unlike Silicon Valley software startups, news organizations don't have one product. Your newspaper and your website are a product, but so is every individual section, every blog, every story. Every desk is a team, and every reporter her own production unit. Bunch all of those metrics together, and it becomes very hard to untangle cause and effect. It also becomes very hard to figure out how good we are at writing things that people like to read and that keeps them coming back.

  - Each story takes a different amount of time to research and write, and so pageviews/story is a naive measurement, pageviews/word is a little bit better and pageviews / hour invested is better still. (This requires you to tie CMS metadata into your analytics) You don't need to be mercenary about this, and in fact, there are so many unknowns and indirect effects that it's probably not a good idea to tie these numbers to performance reviews. But why not just give reporters access to their own numbers? When people get direct feedback on how well they're doing, they improve. No need for intervention.
  - Reporters should have access to the analytics for their stories (as well as popular search terms etc.), so they know what's working and what isn't, and so they can see the direct effect of what they do (e.g. what happens when I spend more time on twitter?) instead of flying blind or relying on business-wide metrics

---
* more fine-grained means it's easier to see cause and effect (e.g. of a new social media strategy). So per-section and per-author. There's a saying that you shouldn't translate your organizational structure to your website ("show your drawers") but for analytics, you explicitly should. If health and science is a single desk, then that's a group you want to track. Pieces written by freelancers should get a group too, to see how they compare.
* I wonder if you can get proxy measures for story effectiveness. Pageviews and tweets, obviously, but maybe also completion rate (how many people read to the end)? Something else?
* It might be interesting to track professional metrics (Did I feel I had enough time to write this? How much time was that? How good do I think this story is?) and then compare that with business metrics (pageviews, shareability) and look at the overlap (how well does an author's confidence in their piece translate into success, and if not, can we take a guess at what's holding them back?)


----

* Don't try to quantify what isn't quantifiable (but you can still measure and find appropriate heuristics!) e.g. for a crowdsourcing project you can make it a habit to always evaluate how well it went and try to distill lessons out of that experience. The fact that you're doing a debrief at all is much more valuable than having hard numbers that can tell you your effort was an 82 on the crowdsourcing scale.


* While I'm really enamored with the idea of measuring impact, I don't think it's a good idea to try and quantify that. But maybe we can combine quantitative metrics that measure mindshare/measurability with heuristics to determine qualitatively how well we think a story did, e.g. with a standardized five-question checklist or questionnaire (Are you happy with the attention this story received? Do you think most people understood what it meant and why this is important? Should we follow up?) Doesn't matter that those things are a bit vague sometimes. "Measure things to improve them" still applies, but there is absolutely no reason to try and reduce those measurement to hard numbers that only tell half the story anyway. So maybe we should mix those together?
* As Daniel Bachhuber suggested, maybe you can poll your readers and ask them simple questions sometimes, do determine how many people in your audience know the basics of a certain issue. (Or, as Daniel suggested, use it to determine whether people see or don't see intro grafs to a story.)



* Writers should be able to compare their performance to their peers. I don't like the idea of editors harassing reporters when their stats aren't up to snuff, but maybe if a reporter sees that he's doing very poorly on social media compared to his colleagues, that might be something that reporter will think about and do something about.

---

BUG-TRACKING is also really important at the per-story and per-author level. The NYT already does this, but it should really be a matter-of-fact thing at all news organizations. As a reporter, you look at your story dashboard and you see (1) if there are any comments you need to answer, (2) what people are saying about your story on social media, (3) how many of the organization's total audience saw your piece ("penetration") and how many new users it brought, (4) whether anyone is claiming any errors, which you might have to correct and (5) feedback from your editor(s) and peers

Seeing all that sort of information for every story on a day to day basis should make people more reflective about their writing and get better at it, sooner. Lack of feedback kills new reporters -- search for stat.